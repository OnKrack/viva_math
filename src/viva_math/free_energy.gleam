//// Free Energy Principle (FEP) calculations.
////
//// Based on Karl Friston's work (2010, 2019).
//// Free Energy bounds surprise (negative log evidence) and can be decomposed as:
////
//// F = Π · (μ - o)² + D_KL(q || p)
////     ↑              ↑
////     Accuracy       Complexity
////     (weighted      (deviation
////     prediction     from priors)
////     error)
////
//// In VIVA, this is used for interoception - sensing internal state
//// and minimizing "surprise" through prediction.
////
//// References:
//// - Friston (2010) "The free-energy principle: a unified brain theory?"
//// - Parr & Friston (2019) "Generalised free energy and active inference"
//// - Validated by DeepSeek R1 671B (2025)

import gleam/float
import gleam/list
import gleam_community/maths
import viva_math/vector.{type Vec3}

/// Free Energy state for a system.
pub type FreeEnergyState {
  FreeEnergyState(
    /// The free energy value (lower is better)
    free_energy: Float,
    /// Prediction error component (precision-weighted)
    prediction_error: Float,
    /// Complexity/KL divergence component
    complexity: Float,
    /// Precision used for weighting
    precision: Float,
    /// Qualitative feeling based on normalized free energy
    feeling: Feeling,
  )
}

/// Qualitative feeling based on free energy level.
pub type Feeling {
  /// Low free energy - predictions match reality (F < μ - σ)
  Homeostatic
  /// Moderate free energy - slight mismatch (μ - σ ≤ F < μ)
  Surprised
  /// High free energy - significant mismatch (μ ≤ F < μ + σ)
  Alarmed
  /// Very high free energy - system overwhelmed (F ≥ μ + σ)
  Overwhelmed
}

/// Thresholds for feeling classification.
/// Based on system-specific statistics (mean and standard deviation).
pub type FeelingThresholds {
  FeelingThresholds(
    /// Mean free energy (baseline)
    mean: Float,
    /// Standard deviation of free energy
    std_dev: Float,
  )
}

/// Default thresholds calibrated for PAD space.
/// Mean and std_dev derived from typical emotional dynamics.
pub fn default_thresholds() -> FeelingThresholds {
  FeelingThresholds(mean: 0.5, std_dev: 0.3)
}

/// Compute raw prediction error between expected and actual state.
/// Uses squared Euclidean distance (L2 loss).
pub fn prediction_error(expected: Vec3, actual: Vec3) -> Float {
  vector.distance_squared(expected, actual)
}

/// Compute precision-weighted prediction error.
///
/// F_accuracy = Π · (expected - actual)²
///
/// Precision (Π) = 1/variance. Higher precision = more weight on prediction errors.
/// This is critical for biological systems where uncertainty should attenuate errors.
pub fn precision_weighted_prediction_error(
  expected: Vec3,
  actual: Vec3,
  precision: Float,
) -> Float {
  let pe = prediction_error(expected, actual)
  precision *. pe
}

/// Compute KL divergence between Gaussian distributions (closed form).
///
/// CORRECTED per DeepSeek R1 validation - Full KL for Gaussians:
/// D_KL(N(μ₁,σ₁²) || N(μ₂,σ₂²)) = (μ₁ - μ₂)²/(2σ₂²) + (σ₁² - σ₂²)/(2σ₂²) - 1/2
///
/// When variances are equal (σ₁ = σ₂), reduces to: (μ₁ - μ₂)²/(2σ²)
///
/// This measures how much the posterior (current belief) diverges from prior.
pub fn gaussian_kl_divergence(
  posterior_mean: Vec3,
  prior_mean: Vec3,
  variance: Float,
) -> Float {
  let diff_squared = vector.distance_squared(posterior_mean, prior_mean)
  case variance <=. 0.0 {
    True -> 0.0
    False -> diff_squared /. { 2.0 *. variance }
  }
}

/// Full KL divergence between Gaussians with different variances.
///
/// D_KL(N(μ₁,σ₁²) || N(μ₂,σ₂²)) = log(σ₂/σ₁) + (σ₁² + (μ₁-μ₂)²)/(2σ₂²) - 1/2
///
/// This is the complete formula from DeepSeek R1 validation.
pub fn gaussian_kl_divergence_full(
  posterior_mean: Vec3,
  prior_mean: Vec3,
  posterior_variance: Float,
  prior_variance: Float,
) -> Float {
  case posterior_variance <=. 0.0 || prior_variance <=. 0.0 {
    True -> 0.0
    False -> {
      let diff_squared = vector.distance_squared(posterior_mean, prior_mean)

      // log(σ₂) - log(σ₁) = 0.5 * (log(σ₂²) - log(σ₁²))
      // Using separate logs for numerical stability when variances are small
      let log_ratio = case maths.natural_logarithm(prior_variance), maths.natural_logarithm(posterior_variance) {
        Ok(log_prior), Ok(log_posterior) -> { log_prior -. log_posterior } /. 2.0
        _, _ -> 0.0
      }

      // (σ₁² + (μ₁-μ₂)²) / (2σ₂²) term
      let ratio_term = { posterior_variance +. diff_squared } /. { 2.0 *. prior_variance }

      // Full KL: log(σ₂/σ₁) + (σ₁² + (μ₁-μ₂)²)/(2σ₂²) - 1/2
      log_ratio +. ratio_term -. 0.5
    }
  }
}

/// Compute complexity term using KL divergence.
///
/// Complexity = D_KL(q(θ) || p(θ))
///
/// Where q is posterior belief and p is prior belief (homeostatic setpoint).
/// Weight controls the regularization strength.
pub fn complexity(
  current: Vec3,
  baseline: Vec3,
  prior_variance: Float,
) -> Float {
  gaussian_kl_divergence(current, baseline, prior_variance)
}

/// Legacy complexity function for backwards compatibility.
pub fn complexity_weighted(
  current: Vec3,
  baseline: Vec3,
  weight: Float,
) -> Float {
  weight *. vector.distance_squared(current, baseline)
}

/// Compute full Free Energy: F = Π·(μ-o)² + D_KL(q||p)
///
/// ## Parameters
/// - expected: predicted/expected state (μ)
/// - actual: observed/actual state (o)
/// - baseline: prior baseline state (p) - e.g., personality/homeostatic setpoint
/// - precision: inverse variance of predictions (Π)
/// - prior_variance: variance of prior beliefs (for KL term)
pub fn free_energy(
  expected: Vec3,
  actual: Vec3,
  baseline: Vec3,
  precision: Float,
  prior_variance: Float,
) -> Float {
  let accuracy = precision_weighted_prediction_error(expected, actual, precision)
  let cx = complexity(actual, baseline, prior_variance)
  accuracy +. cx
}

/// Compute free energy and return full state with feeling.
/// Uses normalized thresholds for feeling classification.
pub fn compute_state(
  expected: Vec3,
  actual: Vec3,
  baseline: Vec3,
  precision: Float,
  prior_variance: Float,
  thresholds: FeelingThresholds,
) -> FreeEnergyState {
  let accuracy = precision_weighted_prediction_error(expected, actual, precision)
  let cx = complexity(actual, baseline, prior_variance)
  let fe = accuracy +. cx

  FreeEnergyState(
    free_energy: fe,
    prediction_error: accuracy,
    complexity: cx,
    precision: precision,
    feeling: classify_feeling_normalized(fe, thresholds),
  )
}

/// Simplified compute_state with default thresholds and legacy interface.
/// For backwards compatibility.
pub fn compute_state_simple(
  expected: Vec3,
  actual: Vec3,
  baseline: Vec3,
  complexity_weight: Float,
) -> FreeEnergyState {
  let pe = prediction_error(expected, actual)
  let cx = complexity_weighted(actual, baseline, complexity_weight)
  let fe = pe +. cx

  FreeEnergyState(
    free_energy: fe,
    prediction_error: pe,
    complexity: cx,
    precision: 1.0,
    feeling: classify_feeling(fe),
  )
}

/// Classify feeling using normalized thresholds.
///
/// - Homeostatic: F < μ - σ (better than expected)
/// - Surprised: μ - σ ≤ F < μ (slightly worse)
/// - Alarmed: μ ≤ F < μ + σ (worse than average)
/// - Overwhelmed: F ≥ μ + σ (much worse)
pub fn classify_feeling_normalized(
  free_energy: Float,
  thresholds: FeelingThresholds,
) -> Feeling {
  let lower = thresholds.mean -. thresholds.std_dev
  let upper = thresholds.mean +. thresholds.std_dev

  case free_energy {
    fe if fe <. lower -> Homeostatic
    fe if fe <. thresholds.mean -> Surprised
    fe if fe <. upper -> Alarmed
    _ -> Overwhelmed
  }
}

/// Legacy classify_feeling with fixed thresholds.
/// Calibrated for PAD space (max distance ~3.46).
pub fn classify_feeling(free_energy: Float) -> Feeling {
  case free_energy {
    fe if fe <. 0.1 -> Homeostatic
    fe if fe <. 0.5 -> Surprised
    fe if fe <. 1.5 -> Alarmed
    _ -> Overwhelmed
  }
}

/// Update thresholds based on observed free energy history.
/// Uses exponential moving average for online learning.
pub fn update_thresholds(
  current: FeelingThresholds,
  observed_fe: Float,
  alpha: Float,
) -> FeelingThresholds {
  // EMA update for mean
  let new_mean = alpha *. observed_fe +. { 1.0 -. alpha } *. current.mean

  // Update variance estimate
  let diff = observed_fe -. current.mean
  let new_var = alpha *. { diff *. diff } +. { 1.0 -. alpha } *. { current.std_dev *. current.std_dev }

  // Convert variance back to std_dev (sqrt = nth_root with n=2)
  let new_std = case maths.nth_root(new_var, 2) {
    Ok(s) -> s
    Error(_) -> current.std_dev
  }

  FeelingThresholds(mean: new_mean, std_dev: float.max(new_std, 0.01))
}

/// Compute surprise for a single dimension.
///
/// Surprise = -log(p(observation | model))
/// Using Gaussian approximation: surprise ∝ (x - μ)² / (2σ²)
pub fn surprise(expected: Float, observed: Float, sigma: Float) -> Float {
  let diff = observed -. expected
  let sigma_sq = sigma *. sigma
  case sigma_sq <=. 0.0 {
    True -> 0.0
    False -> { diff *. diff } /. { 2.0 *. sigma_sq }
  }
}

/// Active Inference: compute action that minimizes expected free energy.
///
/// This returns the delta to apply to current state to move toward target.
/// Rate controls how quickly to move (0 = no movement, 1 = instant).
pub fn active_inference_delta(
  current: Vec3,
  target: Vec3,
  rate: Float,
) -> Vec3 {
  let diff = vector.sub(target, current)
  vector.scale(diff, rate)
}

/// Precision-weighted prediction error for Vec3.
///
/// Each dimension can have different precision.
/// Returns weighted sum of squared errors.
pub fn precision_weighted_error_vec(
  expected: Vec3,
  actual: Vec3,
  precisions: Vec3,
) -> Float {
  let diff = vector.sub(expected, actual)
  let diff_sq = vector.multiply(diff, diff)
  let weighted = vector.multiply(diff_sq, precisions)
  vector.sum(weighted)
}

/// Estimate precision from recent prediction errors.
///
/// Precision = 1 / variance of errors
/// Higher precision means more reliable predictions.
pub fn estimate_precision(errors: List(Float)) -> Float {
  case list.length(errors) {
    0 -> 1.0
    1 -> 1.0
    n -> {
      let n_float = int_to_float(n)
      let mean = list.fold(errors, 0.0, fn(acc, e) { acc +. e }) /. n_float
      let variance =
        list.fold(errors, 0.0, fn(acc, e) {
          let diff = e -. mean
          acc +. diff *. diff
        })
        /. n_float

      case variance <. 0.001 {
        True -> 100.0  // Very precise
        False -> 1.0 /. variance
      }
    }
  }
}

/// Bayesian belief update: combine prior with likelihood.
///
/// posterior ∝ likelihood × prior
/// Using precision-weighted combination:
/// new_belief = (Π_prior × prior + Π_likelihood × observation) /
///              (Π_prior + Π_likelihood)
pub fn belief_update(
  prior: Float,
  observation: Float,
  precision_prior: Float,
  precision_likelihood: Float,
) -> Float {
  let total_precision = precision_prior +. precision_likelihood
  case total_precision <=. 0.0 {
    True -> prior
    False ->
      { precision_prior *. prior +. precision_likelihood *. observation }
      /. total_precision
  }
}

/// Generalized Free Energy (expected free energy for planning).
///
/// G = ambiguity + risk
/// - ambiguity: expected surprise under model (epistemic value)
/// - risk: KL divergence from preferred outcomes (pragmatic value)
///
/// Used for action selection in active inference.
pub fn generalized_free_energy(
  expected_state: Vec3,
  preferred_state: Vec3,
  uncertainty: Float,
) -> Float {
  let ambiguity = uncertainty
  let risk = vector.distance_squared(expected_state, preferred_state)
  ambiguity +. risk
}

/// Variational Free Energy bound.
///
/// F ≤ -log p(o) + D_KL(q||p)
///
/// The free energy bounds the negative log evidence (surprise).
pub fn variational_bound(
  observation_likelihood: Float,
  kl_divergence: Float,
) -> Float {
  let neg_log_likelihood = case observation_likelihood <=. 0.0 {
    True -> 100.0  // Large surprise for impossible observations
    False -> case maths.natural_logarithm(observation_likelihood) {
      Ok(log_l) -> 0.0 -. log_l
      Error(_) -> 100.0
    }
  }
  neg_log_likelihood +. kl_divergence
}

// Helper: convert int to float
fn int_to_float(n: Int) -> Float {
  case n {
    0 -> 0.0
    1 -> 1.0
    2 -> 2.0
    3 -> 3.0
    4 -> 4.0
    5 -> 5.0
    _ -> {
      let half = n / 2
      let remainder = n - half * 2
      int_to_float(half) *. 2.0 +. int_to_float(remainder)
    }
  }
}
